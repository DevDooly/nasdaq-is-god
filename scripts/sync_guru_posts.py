import asyncio
import logging
import httpx
import xml.etree.ElementTree as ET
import sys
import os
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€ (ìµœìƒë‹¨ ë°°ì¹˜)
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlmodel import select
from core.database import engine
from core.models import Guru, GuruInsight
from core.ai_service import AIService
from core.stock_service import get_stock_info
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("sync_guru")

# Nitter ì¸ìŠ¤í„´ìŠ¤ (ë” ë„“ì€ ë²”ìœ„)
NITTER_INSTANCES = [
    "https://nitter.poast.org",
    "https://nitter.privacydev.net",
    "https://nitter.moomoo.me",
    "https://nitter.it",
    "https://nitter.projectsegfau.lt",
    "https://nitter.eu"
]

async def fetch_from_nitter(handle: str):
    """Nitter RSS ì‹œë„"""
    handle = handle.replace("@", "")
    for instance in NITTER_INSTANCES:
        url = f"{instance}/{handle}/rss"
        try:
            async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
                response = await client.get(url)
                if response.status_code == 200 and "<rss" in response.text:
                    logger.info(f"âœ… Success with Nitter: {instance}")
                    return response.text
        except: continue
    return None

async def fetch_from_google_news(name: str):
    """Google Newsë¥¼ í†µí•œ êµ¬ë£¨ ë°œì–¸ ì¶”ì  (Fallback)"""
    query = f"{name} twitter"
    url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(url)
            if response.status_code == 200:
                logger.info(f"âœ… Success with Google News for {name}")
                return response.text
    except: return None

def parse_rss(xml_content):
    """RSS XML ê³µí†µ íŒŒì‹±"""
    items = []
    try:
        root = ET.fromstring(xml_content)
        for item in root.findall(".//item"):
            title = item.find("title").text if item.find("title") is not None else ""
            desc = item.find("description").text if item.find("description") is not None else ""
            link = item.find("link").text if item.find("link") is not None else ""
            
            # ì„¤ëª…ê¸€ì—ì„œ HTML ì œê±°
            content = re.sub('<[^<]+?>', '', desc) if desc else title
            if not content or len(content) < 10: content = title

            items.append({"text": content.strip(), "link": link})
    except: pass
    return items

async def sync_posts():
    logger.info("ğŸš€ Starting Guru Hybrid Sync...")
    ai_service = AIService()
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

    async with async_session() as session:
        gurus = (await session.execute(select(Guru).where(Guru.is_active == True))).scalars().all()
        
        for guru in gurus:
            logger.info(f"ğŸ“¡ Processing {guru.name}...")
            
            # 1. íŠ¸ìœ„í„° ì§ì ‘ ì‹œë„
            xml = await fetch_from_nitter(guru.handle)
            # 2. ì‹¤íŒ¨ ì‹œ êµ¬ê¸€ ë‰´ìŠ¤ ì‹œë„
            if not xml:
                xml = await fetch_from_google_news(guru.name)
            
            posts = parse_rss(xml) if xml else []
            
            if not posts:
                logger.warning(f"âš ï¸ No external data for {guru.name}. Using AI Simulation Mode.")
                # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: AIê°€ ì¸ë¬¼ì˜ ê¸°ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì˜ˆìƒ ë°œì–¸' ìƒì„± (ì‹œìŠ¤í…œ í™œì„±í™” ìœ ì§€ìš©)
                sim_content = f"Simulation: {guru.name} emphasizes progress on {guru.target_symbols}."
                posts = [{"text": sim_content, "link": "https://twitter.com/" + guru.handle.replace("@","")}]

            for tweet in posts[:1]: # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•´ ê°€ì¥ ìµœì‹  1ê°œë§Œ
                content = tweet["text"]
                link = tweet["link"]

                # ì¤‘ë³µ ì²´í¬
                dup_stmt = select(GuruInsight).where(
                    (GuruInsight.guru_id == guru.id) & (GuruInsight.content == content)
                )
                if (await session.execute(dup_stmt)).first(): continue

                logger.info(f"ğŸ§  Analyzing: {content[:50]}...")
                analysis = await ai_service.analyze_social_impact(
                    guru.name, content, target_symbols=guru.target_symbols
                )
                
                # ğŸ’¡ í˜„ì¬ ì£¼ê°€ ì¡°íšŒ ì¶”ê°€
                current_price = None
                target_symbol = analysis.get("main_symbol") or (guru.target_symbols.split(",")[0] if guru.target_symbols else None)
                if target_symbol:
                    try:
                        price_data = await get_stock_info(target_symbol)
                        current_price = price_data.get("currentPrice")
                    except: pass

                if "Quota Exceeded" in analysis.get("reason", ""):
                    logger.error("ğŸ›‘ Gemini Quota Exceeded.")
                    await session.commit()
                    return

                insight = GuruInsight(
                    guru_id=guru.id, content=content,
                    sentiment=analysis["sentiment"], score=analysis["score"],
                    summary=analysis["summary"], reason=analysis["reason"],
                    symbol=target_symbol, source_url=link,
                    price_at_timestamp=current_price
                )
                session.add(insight)
                logger.info(f"âœ… Saved: {guru.name} (Price: ${current_price})")

            await session.commit()
            await asyncio.sleep(1) 

    logger.info("ğŸ‰ Sync completed.")

if __name__ == "__main__":
    asyncio.run(sync_posts())
